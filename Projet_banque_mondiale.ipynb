{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importations des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pysheds.grid import Grid\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import cv2\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversion de la zone de travail en raster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation carte\n",
    "path = \"merged_map.tif\" #Chemin d'accès vers le dem au format .tif. On télécharge plusieurs dem au format .tif qui couvrent toute\n",
    "                        #la région de travail sur USGS puis on merge ces dem avec ARCGIS ou QGIS pour obtenir un grand dem qui \n",
    "                        #couvre toute la région de d'étude.\n",
    "\n",
    "grid = Grid.from_raster(path, data_name=\"dem\")\n",
    "dem = grid.read_raster(path)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "fig.patch.set_alpha(0)\n",
    "\n",
    "plt.imshow(grid.view(dem), cmap='terrain', zorder=1)\n",
    "plt.colorbar(label='Elevation (m)')\n",
    "plt.title('Digital elevation map', size=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création des cartes constantes -> carte des denivelés filled, des directions, des accumulations et des gradients\n",
    "# WARNING : Met plusieurs minutes à tourner (6min sur mon ordinateur)\n",
    "pit_filled_dem = grid.fill_pits(dem)\n",
    "flooded_dem = grid.fill_depressions(pit_filled_dem)\n",
    "inflated_dem = grid.resolve_flats(flooded_dem) #dem\n",
    "fdir = grid.flowdir(inflated_dem) #carte des directions. Numpy array de même taille que le dem. Pour chaque case donne la direction\n",
    "                                  #vers laquelle irait une goutte d'eau à partir de ce point. Encodage ordinal (les directions sont\n",
    "                                  #représentées par des puissances de 2)\n",
    "acc = grid.accumulation(fdir) #carte des accumulations. Numpy array de même taille que le dem. Pour chaque case donne la valeur\n",
    "                              #de l'accumulation en ce point\n",
    "\n",
    "kernel_size = (5, 5)\n",
    "sigma = 1.0\n",
    "inflated_dem = cv2.GaussianBlur(inflated_dem, kernel_size, sigma) #Pour lisser le dem\n",
    "\n",
    "grads = np.gradient(inflated_dem)\n",
    "grad_x = grads[0]\n",
    "grad_y = grads[1]\n",
    "grad = np.sqrt(grad_x**2 + grad_y**2) #Carte des pentes. Numpy array de même taille que le dem. Pour chaque case donne la pente en ce point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optionnel: pour sauvegarder les cartes calculées pour ne pas refaire le calcul à chaque fois\n",
    "np.save(\"inflated_dem.npy\", inflated_dem)\n",
    "np.save(\"fdir.npy\", fdir)\n",
    "np.save(\"acc.npy\", acc)\n",
    "np.save(\"grad.npy\", grad)\n",
    "\n",
    "with open('grid_affine.pkl', 'wb') as f:\n",
    "    pickle.dump(grid.affine, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Etude du terrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si on a sauvegardé les cartes calculées précédemment, on peut les recharger à partir de cette cellule, donc plus besoin\n",
    "#des cellules 2, 3 et 4.\n",
    "inflated_dem = np.load('inflated_dem.npy')\n",
    "fdir = np.load('fdir.npy')\n",
    "acc = np.load('acc.npy')\n",
    "\n",
    "with open('grid_affine.pkl', 'rb') as f:\n",
    "    grid_affine = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On importe un des excels contenant les profondeurs des nappes\n",
    "df = pd.read_excel(\"Mali.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonctions utiles pour calculer les features\n",
    "\n",
    "def convert_coord_to_indices(lon, lat):\n",
    "    #Transforme des coordonnées longitude, latitude en coordonnées dans le raster (colonne, ligne)\n",
    "    col = round((lon - grid_affine[2]) / grid_affine[0])\n",
    "    row = round((lat - grid_affine[5]) / grid_affine[4])\n",
    "    return col, row\n",
    "\n",
    "def find_paths(map_direction, start_point):\n",
    "    #Prend la carte des directions \"fdir\" et un point de départ (ligne, colonne) qui représente la sortie\n",
    "    #d'un bassin versant. La fonction retourne les coordonnées de tous les points de ce bassin versant\n",
    "    #Elle permet de mapper tout un bassin versant à partir d'un point de sortie\n",
    "    dir_map = {64: (-1, 0), \n",
    "                    128: (-1, 1), \n",
    "                    1: (0, 1), \n",
    "                    2: (1, 1), \n",
    "                    4: (1, 0), \n",
    "                    8: (1, -1), \n",
    "                    16: (0, -1), \n",
    "                    32: (-1, -1)}\n",
    "    def is_valid_move(x, y):\n",
    "        return 0 <= x < map_direction.shape[0] and 0 <= y < map_direction.shape[1] and not visited[x, y]\n",
    "\n",
    "    def dfs(x, y, path):\n",
    "        visited[x, y] = True\n",
    "        for dx, dy in [(1, 0), (-1, 0), (0, 1), (0, -1), (-1, 1), (1, -1), (1, 1), (-1, -1)]:\n",
    "            new_x, new_y = x + dx, y + dy\n",
    "            if is_valid_move(new_x, new_y) and dir_map.get(map_direction[new_x, new_y], (0, 0)) == (-dx, -dy):\n",
    "                dfs(new_x, new_y, path + [(new_x, new_y)])\n",
    "        paths.append(path)\n",
    "        visited[x, y] = False\n",
    "        return\n",
    "\n",
    "    paths = []\n",
    "    visited = np.zeros(map_direction.shape, dtype=bool)\n",
    "    dfs(*start_point, [start_point])\n",
    "    return paths\n",
    "\n",
    "def contour_custom(mask):\n",
    "    #Prend un mask (un tableau numpy de 0 et de 1 avec les 1 qui forment un ensemble plein, comme un bassin versant, et retourne\n",
    "    #le contour)\n",
    "    new_mask = np.zeros_like(mask)\n",
    "    for i, j in zip(np.where(mask==1)[0], np.where(mask==1)[1]):\n",
    "        if mask[i, j]==1 and (mask[i+1, j]!=1 or mask[i-1, j]!=1 or mask[i, j+1]!=1 or mask[i, j-1]!=1):\n",
    "            new_mask[i, j]=1\n",
    "    return new_mask\n",
    "\n",
    "def move_one_step(col, row, fdir):\n",
    "    #Trouve la case où irait une goutte d'eau à partir des coordonnées (ligne, colonne) et de la carte des directions \"fdir\"\n",
    "    direction = fdir[row, col]\n",
    "\n",
    "    delta_dir = {\n",
    "        64: (-1, 0),\n",
    "        128: (-1, 1),\n",
    "        1: (0, 1),\n",
    "        2: (1, 1),\n",
    "        4: (1, 0),\n",
    "        8: (1, -1),\n",
    "        16: (0, -1),\n",
    "        32: (-1, -1),\n",
    "    }.get(direction, (0, 0))\n",
    "    return col + delta_dir[1], row + delta_dir[0]\n",
    "\n",
    "def find_stream(row, col, acc, threshold, fdir):\n",
    "    #Descend la pente à partir d'un point (ligne, colonne) et renvoie tout le chemin jusqu'à croiser un cours d'eau\n",
    "    new_col, new_row = col, row\n",
    "    consecutive_count = 0\n",
    "    res_x = []\n",
    "    res_y = []\n",
    "    while consecutive_count < 1:\n",
    "        if acc[new_row, new_col] >= threshold:\n",
    "            consecutive_count += 1\n",
    "        else:\n",
    "            consecutive_count = 0\n",
    "        res_x.append(new_col)\n",
    "        res_y.append(new_row)\n",
    "        new_col, new_row = move_one_step(new_col, new_row, fdir=fdir)\n",
    "    return res_x, res_y\n",
    "\n",
    "def bresenham_line(x0, y0, x1, y1):\n",
    "    #Trace une ligne directe entre deux coordonnées spécifiés\n",
    "    points = []\n",
    "    dx = abs(x1 - x0)\n",
    "    dy = abs(y1 - y0)\n",
    "    sx = 1 if x0 < x1 else -1\n",
    "    sy = 1 if y0 < y1 else -1\n",
    "    err = dx - dy\n",
    "    \n",
    "    while True:\n",
    "        points.append((x0, y0))\n",
    "        if x0 == x1 and y0 == y1:\n",
    "            break\n",
    "        e2 = err * 2\n",
    "        if e2 > -dy:\n",
    "            err -= dy\n",
    "            x0 += sx\n",
    "        if e2 < dx:\n",
    "            err += dx\n",
    "            y0 += sy\n",
    "            \n",
    "    return points\n",
    "\n",
    "def good_side(path, acc, threshold):\n",
    "    #Vérifie si le chemin donné (path) passe par un cours d'eau ou non\n",
    "    for col, row in path:\n",
    "        if acc[row, col] > threshold:\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(outlet: tuple, threshold: int=1500):\n",
    "    #Prend un outlet (coordonnée (ligne, colonne) de sortie d'un bassin versant) et affiche en 3d la région aux alentours (dem) avec le\n",
    "    #contour du bassin versant calculé en noir, l'outlet en rouge, et les cours d'eau calculés à partir d'un seuillage de l'accumulation\n",
    "    #(à 1500 par défaut) en bleu.    \n",
    "    row_min = outlet[0] - 200\n",
    "    row_max = outlet[0] + 200\n",
    "    col_min = outlet[1] - 200\n",
    "    col_max = outlet[1] + 200\n",
    "\n",
    "    paths = find_paths(fdir, outlet)\n",
    "    flattened_list = {item for sublist in paths for item in sublist}\n",
    "    catch = np.zeros_like(inflated_dem)\n",
    "    for element in flattened_list:\n",
    "        catch[element]=1\n",
    "    contour_catch = np.where(contour_custom(catch)==1)\n",
    "    contour_catch_x = contour_catch[1]\n",
    "    contour_catch_y = contour_catch[0]\n",
    "    contour_catch_z = [inflated_dem[row, col] for row, col in zip(contour_catch_y, contour_catch_x)]\n",
    "\n",
    "    col_coords = [outlet[1]]\n",
    "    row_coords = [outlet[0]]\n",
    "    z_coords = [inflated_dem[row, col] for col, row in zip(col_coords, row_coords)]\n",
    "\n",
    "    region_cols, region_rows = np.meshgrid(\n",
    "        np.arange(col_min, col_max),\n",
    "        np.arange(row_min, row_max)\n",
    "    )\n",
    "    region_points = np.vstack([region_cols.ravel(), region_rows.ravel()]).T\n",
    "\n",
    "    threshold_points = [\n",
    "        (col, row) for col, row in region_points\n",
    "        if acc[row, col] > threshold\n",
    "    ]\n",
    "    threshold_col_coords = [col for col, row in threshold_points]\n",
    "    threshold_row_coords = [row for col, row in threshold_points]\n",
    "    threshold_z_coords = [inflated_dem[row, col] for col, row in threshold_points]\n",
    "\n",
    "    surface_x = np.linspace(col_min, col_max, col_max - col_min + 1)\n",
    "    surface_y = np.linspace(row_min, row_max, row_max - row_min + 1)\n",
    "    surface_z = inflated_dem[row_min: row_max, col_min: col_max]\n",
    "\n",
    "    fig = go.Figure(data=[\n",
    "        go.Surface(x=surface_x, y=surface_y, z=surface_z, colorscale='Viridis', opacity=1),\n",
    "        go.Scatter3d(x=threshold_col_coords, y=threshold_row_coords, z=threshold_z_coords, mode='markers', marker=dict(color='blue', size=5), name='Accumulation > Threshold'),\n",
    "        go.Scatter3d(x=contour_catch_x, y=contour_catch_y, z=contour_catch_z, mode='markers', marker=dict(color='black', size=5), name='Contour catchment'),\n",
    "        go.Scatter3d(x=col_coords, y=row_coords, z=z_coords, mode='markers', marker=dict(color='red', size=8)),\n",
    "    ])\n",
    "\n",
    "    fig.update_layout(scene=dict(\n",
    "                        xaxis_title='Longitude',\n",
    "                        yaxis_title='Latitude',\n",
    "                        zaxis_title='Z (Planche)'\n",
    "                    ),\n",
    "                    autosize=False,\n",
    "                    scene_aspectmode='data')\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet = (5269, 4183)\n",
    "visualize(outlet=outlet) #Graphe 3d interactif (on peut se déplacer à l'intérieur, zoomer, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.copy() #dataframe dans lequel on va mettre les features calculées\n",
    "dataset[\"top\"] = pd.Series(index=dataset.index, dtype='object')\n",
    "dataset[\"stream\"] = pd.Series(index=dataset.index, dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(outlet: tuple, threshold: int=1500):\n",
    "    #Prend un outlet (coordonnée (ligne, colonne) de sortie d'un bassin versant) et calcule le point de crête le plus proche (non pas\n",
    "    #le plus proche exactement, mais celui dont le chemin qui mène jusqu'au cours d'eau passe le plus près de notre point), le point du cours\n",
    "    #d'eau (non pas le plus proche du point mais le croisement que l'on obtient en descendant la pente, on va le changer pour mettre le\n",
    "    #point le plus proche) pour chaque point du jeu de données (df) qui est contenu dans ce bassin versant puis calcule tout un tas de\n",
    "    #features pour ces-mêmes points \n",
    "    paths = find_paths(fdir, outlet)\n",
    "    flattened_list = {item for sublist in paths for item in sublist}\n",
    "    catch = np.zeros_like(inflated_dem)\n",
    "    for element in flattened_list:\n",
    "        catch[element]=1\n",
    "    contour_catch = np.where(contour_custom(catch)==1)\n",
    "    contour_catch_x = contour_catch[1]\n",
    "    contour_catch_y = contour_catch[0]\n",
    "    contour_catchment = [(row, col) for row, col in zip(contour_catch_y, contour_catch_x)] #Les coordonnées du contour du bassin versant, sous\n",
    "                                                                                           #la forme d'une liste de tuples\n",
    "    for i in df.index:\n",
    "        col, row = convert_coord_to_indices(df.loc[i, \"LON\"], df.loc[i, \"LAT\"])\n",
    "        point = (row, col)\n",
    "        if catch[row, col]==1:\n",
    "            res_top = None\n",
    "            inter_dist = np.inf\n",
    "            for catch_row, catch_col in contour_catchment:\n",
    "                res_x, res_y = find_stream(row=catch_row, col=catch_col, acc=acc, threshold=threshold, fdir=fdir)\n",
    "                path_to_stream = [(row, col) for col, row in zip(res_x, res_y)]\n",
    "                distance_minimale = min([np.sqrt((row - point[0])**2 + (col - point[1])**2) for row, col in path_to_stream])\n",
    "                if distance_minimale < inter_dist:\n",
    "                    inter_dist = distance_minimale\n",
    "                    res_top = (catch_row, catch_col)\n",
    "            res_x, res_y = find_stream(row=point[0], col=point[1], acc=acc, threshold=threshold, fdir=fdir)\n",
    "            res_stream = (res_y[-1], res_x[-1])\n",
    "            dataset.at[i, \"top\"] = res_top\n",
    "            dataset.at[i, \"stream\"] = res_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlet = (5269, 4183)\n",
    "pipeline(outlet=outlet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset[\"top\"]==dataset[\"top\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_distribution(array, coordinates, feature_name):\n",
    "    coord_array = array[coordinates[:, 1], coordinates[:, 0]]\n",
    "    return {\n",
    "        f\"distrib_{feature_name}_mean\": np.mean(coord_array),\n",
    "        f\"distrib_{feature_name}_var\": np.var(coord_array),\n",
    "        f\"distrib_{feature_name}_skew\": stats.skew(coord_array),\n",
    "        f\"distrib_{feature_name}_kurt\": stats.kurtosis(coord_array),\n",
    "        f\"distrib_{feature_name}_max\": np.max(coord_array),\n",
    "    } \n",
    "\n",
    "def test_feature(index):\n",
    "    if dataset.loc[index, \"top\"]!=dataset.loc[index, \"top\"] or dataset.loc[index, \"stream\"]!=dataset.loc[index, \"stream\"]:\n",
    "        raise Exception(\"we don't have top_point or stream point for this index\")\n",
    "    col_point, row_point = convert_coord_to_indices(lon=df.loc[index, \"LON\"], lat=df.loc[index, \"LAT\"])\n",
    "    row_closest_stream, col_closest_stream = dataset.loc[index, \"stream\"]\n",
    "    stream_points = bresenham_line(x0=col_point, y0=row_point, x1=col_closest_stream, y1=row_closest_stream)\n",
    "    \n",
    "    row_closest_top, col_closest_top = dataset.loc[index, \"top\"]\n",
    "    top_points = bresenham_line(x0=col_point, y0=row_point, x1=col_closest_top, y1=row_closest_top)\n",
    "    if not good_side(path=top_points, acc=acc, threshold=1500):\n",
    "        raise Exception(\"Ridge point not on the good side of the stream\")\n",
    "    \n",
    "    #calculate features\n",
    "    res = {\n",
    "        \"alt_stream\": inflated_dem[row_point, col_point] - inflated_dem[row_closest_stream, col_closest_stream],\n",
    "        \"dist_stream\": np.sqrt((row_point - row_closest_stream)**2 + (col_point - col_closest_stream)**2)*30,\n",
    "        \"alt_top\": inflated_dem[row_closest_top, col_closest_top] - inflated_dem[row_point, col_point],\n",
    "        \"dist_top\": np.sqrt((row_point - row_closest_top)**2 + (col_point - col_closest_top)**2)*30,\n",
    "        \"ratio_alt\": (inflated_dem[row_point, col_point] - inflated_dem[row_closest_stream, col_closest_stream]) / (inflated_dem[row_closest_top, col_closest_top] - inflated_dem[row_point, col_point]),\n",
    "        \"ratio_dist\": (np.sqrt((row_point - row_closest_stream)**2 + (col_point - col_closest_stream)**2)) / (np.sqrt((row_point - row_closest_top)**2 + (col_point - col_closest_top)**2)),\n",
    "        \"ratio_stream\": (inflated_dem[row_point, col_point] - inflated_dem[row_closest_stream, col_closest_stream]) / (np.sqrt((row_point - row_closest_stream)**2 + (col_point - col_closest_stream)**2)*30),\n",
    "        \"ratio_top\": (inflated_dem[row_closest_top, col_closest_top] - inflated_dem[row_point, col_point]) / (np.sqrt((row_point - row_closest_top)**2 + (col_point - col_closest_top)**2)*30),\n",
    "        \"altitude\": inflated_dem[row_point, col_point],\n",
    "        \"accumulation\": acc[row_point, col_point],\n",
    "           }\n",
    "\n",
    "    stream_line_points = np.array([[row, col] for row, col in stream_points])\n",
    "\n",
    "    res.update(return_distribution(array=grad, coordinates=stream_line_points, feature_name=\"grad_stream\"))\n",
    "    res.update(return_distribution(array=acc, coordinates=stream_line_points, feature_name=\"acc_stream\"))\n",
    "\n",
    "    top_line_points = np.array([[row, col] for row, col in top_points])\n",
    "\n",
    "    res.update(return_distribution(array=grad, coordinates=top_line_points, feature_name=\"grad_top\"))\n",
    "    res.update(return_distribution(array=acc, coordinates=top_line_points, feature_name=\"acc_top\"))\n",
    "    \n",
    "    center_x, center_y = col_point, row_point\n",
    "    max_short_distance = 5\n",
    "    max_long_distance = 50\n",
    "    x = np.arange(inflated_dem.shape[1])\n",
    "    y = np.arange(inflated_dem.shape[0])\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    distances = np.sqrt((X - center_x) ** 2 + (Y - center_y) ** 2)\n",
    "\n",
    "    points_within_short_distance = np.argwhere(distances <= max_short_distance)\n",
    "\n",
    "    res.update(return_distribution(array=grad, coordinates=points_within_short_distance, feature_name=\"grad_close\"))\n",
    "    res.update(return_distribution(array=acc, coordinates=points_within_short_distance, feature_name=\"acc_close\"))\n",
    "    \n",
    "    points_within_long_distance = np.argwhere(distances <= max_long_distance)\n",
    "\n",
    "    res.update(return_distribution(array=grad, coordinates=points_within_long_distance, feature_name=\"grad_large\"))\n",
    "    res.update(return_distribution(array=acc, coordinates=points_within_long_distance, feature_name=\"acc_large\"))\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 0\n",
    "for index in dataset.index:\n",
    "    num +=1\n",
    "    print(f\"Point {num}/{len(dataset)}\")\n",
    "    try:\n",
    "        calcul_features = test_feature(index=index)\n",
    "        for feature in calcul_features.keys():\n",
    "            if feature not in dataset.columns:\n",
    "                if type(calcul_features[feature])=='int':\n",
    "                    series_type = \"int\"\n",
    "                elif type(calcul_features[feature])=='float':\n",
    "                    series_type = \"float\"\n",
    "                else:\n",
    "                    series_type = \"object\"\n",
    "                dataset[feature] = pd.Series(dtype=series_type)\n",
    "            dataset.at[index, feature] = calcul_features[feature]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset[\"top\"]==dataset[\"top\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Si on dispose d'une liste d'outlets de bassins versants, on peut donc simplement run les lignes suivantes pour calculer les\n",
    "#features sur tous les points dont on dispose qui sont dans les bassins versants dont on a trouvé l'outlet manuellement\n",
    "\n",
    "list_outlets = [(4710, 7665), (9809, 10720), (2594, 4171), (8140, 10088), (4622, 2960),  (4636, 3121), (7549, 7042)]\n",
    "\n",
    "for outlet in list_outlets:\n",
    "    pipeline(outlet=outlet)\n",
    "\n",
    "num = 0\n",
    "for index in dataset.index:\n",
    "    num +=1\n",
    "    print(f\"Point {num}/{len(dataset)}\")\n",
    "    try:\n",
    "        calcul_features = test_feature(index=index)\n",
    "        for feature in calcul_features.keys():\n",
    "            if feature not in dataset.columns:\n",
    "                if type(calcul_features[feature])=='int':\n",
    "                    series_type = \"int\"\n",
    "                elif type(calcul_features[feature])=='float':\n",
    "                    series_type = \"float\"\n",
    "                else:\n",
    "                    series_type = \"object\"\n",
    "                dataset[feature] = pd.Series(dtype=series_type)\n",
    "            dataset.at[index, feature] = calcul_features[feature]\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[dataset[\"top\"]==dataset[\"top\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On peut afficher les corrélations (inutile ici avec seulement 50 lignes pour l'exemple)\n",
    "\n",
    "plt.figure(figsize=(50, 50))\n",
    "sns.heatmap(dataset[dataset[\"top\"]==dataset[\"top\"]].dropna().drop([\"CODE_OUVRA\", \"LON\", \"LAT\", \"DATE\", \"top\", \"stream\"], axis=1).corr(), annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enfin, on peut entrainer notre modèle et voir ses performances (inutile ici avec seulement 50 lignes pour l'exemple)\n",
    "\n",
    "X = dataset[dataset[\"top\"]==dataset[\"top\"]].dropna().drop([\"CODE_OUVRA\", \"LON\", \"LAT\", \"DATE\", \"top\", \"stream\", \"NS\"], axis=1).astype(\"float\")\n",
    "y = dataset.loc[X.index, \"NS\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestRegressor(n_estimators=100)\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"RMSE (en %): {round(np.sqrt(np.mean((y_test - y_pred)**2))/np.mean(y_test) * 100, 3)} %\")\n",
    "print(f\"Corrélation entre les prédictions et les vraies valeurs: {np.corrcoef(y_test, y_pred)[0, 1].round(3)*100} %\")\n",
    "\n",
    "max_limit = max(20, 20)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(y_test, y_pred)\n",
    "\n",
    "T = np.arange(0, max_limit + 1)\n",
    "plt.plot(T, T, 'r')\n",
    "\n",
    "plt.xlim(0, max_limit)\n",
    "plt.ylim(0, max_limit)\n",
    "\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.xlabel('True values')\n",
    "plt.ylabel('Predicted values')\n",
    "plt.title('True vs Predicted values')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Worldbank-30dnQkkI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
